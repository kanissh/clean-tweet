{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet):\n",
    "    \n",
    "    # remove leading and trailing whitespaces\n",
    "    clean_tweet = tweet.strip()\n",
    "\n",
    "    # remove RT\n",
    "    clean_tweet = re.sub('^RT[\\s]+', '', clean_tweet)\n",
    "\n",
    "    # remove mentions\n",
    "    clean_tweet = re.sub('@[A-Za-z0-9\\_]+', '', clean_tweet)\n",
    "\n",
    "    # remove hyperlinks\n",
    "    clean_tweet = re.sub('https?://\\S+', '', clean_tweet)\n",
    "\n",
    "    #remove hash from hashtag\n",
    "    clean_tweet = re.sub('\\#', '', clean_tweet)\n",
    "\n",
    "    #remove single numeric terms and english characters\n",
    "    clean_tweet = re.sub('[0-9A-Za-z]', '', clean_tweet)\n",
    "\n",
    "    # remove puncttuation except ?\n",
    "    clean_tweet = re.sub('[\\!\\(\\)\\-\\[\\]\\{\\}\\;\\:\\'\\\"\\,\\<\\>\\.\\\\\\/\\@\\#\\$\\%\\^\\&\\*\\_\\~\\|\\=\\-\\+]', ' ', clean_tweet)\n",
    "    clean_tweet = re.sub('\\?{2,}', '?', clean_tweet)\n",
    "\n",
    "    # remove all emojis\n",
    "    clean_tweet = emoji.get_emoji_regexp().sub('', clean_tweet)\n",
    "\n",
    "    # remove extra spaces\n",
    "    clean_tweet = re.sub('\\s{2,}', ' ', clean_tweet)\n",
    "\n",
    "    # remove leading and trailing whitespaces\n",
    "    clean_tweet = clean_tweet.strip()\n",
    "\n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_reduced_dataset = 'F:/FOS/400L/cs421-research-project/scrape-data/clean-dataset-si/param-reduced-dataset'\n",
    "reply_removed_dataset = 'F:/FOS/400L/cs421-research-project/scrape-data/clean-dataset-si/reply-removed-dataset'\n",
    "\n",
    "def pp_file(read_file_path, write_file_path):\n",
    "\n",
    "    pp_write_fp = open(write_file_path,'a')\n",
    "    pp_read_fp = open(read_file_path)\n",
    "\n",
    "    with pp_read_fp as f:\n",
    "        for line in tqdm(f):\n",
    "            raw_tweet = json.loads(line)\n",
    "            inReplyToTweetId = raw_tweet['inReplyToTweetId']\n",
    "\n",
    "            if inReplyToTweetId == None:\n",
    "                pp_write_fp.write(f'{json.dumps(raw_tweet)}\\n')         \n",
    "\n",
    "    pp_write_fp.close()\n",
    "    pp_read_fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_dataset(path_to_folder, output_folder_path):\n",
    "    path = os.walk(path_to_folder)\n",
    "    for root, directories, files in path:\n",
    "        for file in tqdm(files):\n",
    "            pp_file(f'{path_to_folder}/{file}', f'{output_folder_path}/rr-{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "370121it [00:36, 10101.15it/s], ?it/s]\n",
      "236851it [00:39, 5945.27it/s]12:12, 36.65s/it]\n",
      "239241it [00:27, 8588.00it/s]12:12, 38.53s/it]\n",
      "236684it [00:35, 6757.07it/s] 0:05, 33.66s/it]\n",
      "255515it [00:25, 9893.09it/s]09:41, 34.23s/it]\n",
      "173256it [00:41, 4192.53it/s]08:19, 31.22s/it]\n",
      "226257it [00:34, 6498.68it/s]08:40, 34.73s/it]\n",
      "216724it [00:32, 6607.25it/s]08:06, 34.76s/it]\n",
      "248438it [00:18, 13795.25it/s]7:24, 34.15s/it]\n",
      "242440it [00:17, 13639.21it/s]5:49, 29.11s/it]\n",
      "204637it [00:24, 8281.20it/s]<04:41, 25.61s/it]\n",
      "267790it [00:37, 7161.80it/s] 04:13, 25.34s/it]\n",
      "242554it [00:27, 8780.83it/s] 04:21, 29.02s/it]\n",
      "308563it [00:57, 5376.15it/s]<03:48, 28.60s/it]\n",
      "226503it [00:34, 6536.32it/s] 04:21, 37.30s/it]\n",
      "308729it [00:41, 7364.66it/s] 03:39, 36.50s/it]\n",
      "261144it [00:41, 6344.06it/s] 03:10, 38.15s/it]\n",
      "254849it [00:34, 7363.09it/s]<02:36, 39.06s/it]\n",
      "272587it [00:19, 14234.64it/s]01:53, 37.73s/it]\n",
      "213103it [00:23, 8967.11it/s] 01:04, 32.15s/it]\n",
      "219761it [00:38, 5699.06it/s]<00:29, 29.64s/it]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21/21 [11:31<00:00, 32.93s/it]\n"
     ]
    }
   ],
   "source": [
    "pp_dataset(param_reduced_dataset,reply_removed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "# test_wfp = open()\n",
    "\n",
    "with open(f'{reply_removed_dataset}/rr-clean-dec20.json') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        print(tweet['content'])\n",
    "        print('----------------')\n",
    "        count += 1\n",
    "\n",
    "        if(count == 100):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_tweet = 'RT @________ http://4sh.com/samqUI @qAziteeeehen à¶‘à¶­à¶šà·œà¶§ à¶¯à·à¶±à·Š à¶‰à¶±à·Šà¶± à¶¸à·“ à·„à¶»à¶šà¶§ à¶¸à·œà¶šà¶¯ à¶šà¶»à¶±à·Šà¶±à·™ 6000 à·ƒà·à¶¢à¶±à·Š 5000à¶…à¶ºà·Šà¶ºà·š ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ðŸ¤£ #thiswasahashtag ???????[\\!\\(\\)\\-\\[\\]\\{\\}\\;\\:\\'\\\"\\,\\<\\>\\.\\/\\@\\#\\$\\%\\^\\&\\*\\_\\~\\|]'\n",
    "\n",
    "# # remove leading and trailing whitespaces\n",
    "# clean_tweet = clean_tweet.strip()\n",
    "\n",
    "# # remove RT\n",
    "# clean_tweet = re.sub('^RT[\\s]+', '', clean_tweet)\n",
    "\n",
    "# # remove mentions\n",
    "# clean_tweet = re.sub('@[A-Za-z0-9\\_]+', '', clean_tweet)\n",
    "\n",
    "# # remove hyperlinks\n",
    "# clean_tweet = re.sub('https?://\\S+', '', clean_tweet)\n",
    "\n",
    "# #remove hash from hashtag\n",
    "# clean_tweet = re.sub('\\#', '', clean_tweet)\n",
    "\n",
    "# #remove single numeric terms and english characters\n",
    "# clean_tweet = re.sub('[0-9A-Za-z]', '', clean_tweet)\n",
    "\n",
    "# # remove puncttuation except ?\n",
    "# clean_tweet = re.sub('[\\!\\(\\)\\-\\[\\]\\{\\}\\;\\:\\'\\\"\\,\\<\\>\\.\\\\\\/\\@\\#\\$\\%\\^\\&\\*\\_\\~\\|\\=\\-\\+]', '', clean_tweet)\n",
    "# clean_tweet = re.sub('\\?{2,}', '?', clean_tweet)\n",
    "\n",
    "\n",
    "# # remove all emojis\n",
    "# clean_tweet = emoji.get_emoji_regexp().sub('', clean_tweet)\n",
    "\n",
    "# # remove extra spaces\n",
    "# clean_tweet = re.sub('\\s{2,}', ' ', clean_tweet)\n",
    "\n",
    "# # remove leading and trailing whitespaces\n",
    "# clean_tweet = clean_tweet.strip()\n",
    "\n",
    "# print(clean_tweet)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbe5e7158a683de7fe8fed4b69e8361070de09717eccfe05e8e19bc613d24a47"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
